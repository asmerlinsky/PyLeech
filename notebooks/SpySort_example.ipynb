{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Agustin\\\\Dropbox\\\\Doctorado\\\\NeuroData\\\\PyLeech'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Locust_1.dat.gz', <http.client.HTTPMessage at 0x20f37a6a550>),\n",
       " ('Locust_2.dat.gz', <http.client.HTTPMessage at 0x20f37a6a780>),\n",
       " ('Locust_3.dat.gz', <http.client.HTTPMessage at 0x20f37a6a9b0>),\n",
       " ('Locust_4.dat.gz', <http.client.HTTPMessage at 0x20f37a6abe0>)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sorting_with_python as swp\n",
    "from urllib.request import urlretrieve # Python 3\n",
    "# from urllib import urlretrieve # Python 2\n",
    "data_names = ['Locust_' + str(i) + '.dat.gz' for i in range(1,5)]\n",
    "data_src = ['http://xtof.disque.math.cnrs.fr/data/' + n\n",
    "            for n in data_names]\n",
    "[urlretrieve(data_src[i],data_names[i]) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import system\n",
    "[system(\"gunzip \" + fn) for fn in data_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import sorting_with_python as swp\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000\n"
     ]
    }
   ],
   "source": [
    "# Create a list with the file names\n",
    "data_files_names = ['Locust_' + str(i) + '.dat' for i in range(1,5)]\n",
    "# Get the lenght of the data in the files\n",
    "data_len = np.unique(list(map(len, map(lambda n:\n",
    "                                       np.fromfile(n,np.double),\n",
    "                                       data_files_names))))[0]\n",
    "# Load the data in a list of numpy arrays\n",
    "data = [np.fromfile(n,np.double) for n in data_files_names]\n",
    "print(data_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should start by getting an overall picture of the data like the one provided by the mquantiles method of module scipy.stats.mstats using it to output a five-number summary. The five numbers are the minimum, the first quartile, the median, the third quartile and the maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-9.074, -0.371, -0.029,  0.326, 10.626]),\n",
       " array([-8.229, -0.45 , -0.036,  0.396, 11.742]),\n",
       " array([-6.89 , -0.53 , -0.042,  0.469,  9.849]),\n",
       " array([-7.347, -0.492, -0.04 ,  0.431, 10.564])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats.mstats import mquantiles\n",
    "np.set_printoptions(precision=3)\n",
    "[mquantiles(x,prob=[0,0.25,0.5,0.75,1]) for x in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check next if some processing like a division by the standard deviation (SD) has been applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9999983333319417,\n",
       " 0.9999983333319362,\n",
       " 0.9999983333319479,\n",
       " 0.9999983333317428]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.std(x) for x in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily obtain the size of the digitization set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.006709845078411547,\n",
       " 0.009194500187932775,\n",
       " 0.011888432902217971,\n",
       " 0.009614042128660572]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.min(np.diff(np.sort(np.unique(x)))) for x in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = np.arange(0,data_len)/1.5e4\n",
    "%matplotlib qt5\n",
    "plt.figure()\n",
    "swp.plot_data_list(data,tt,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'locust-sorting-python/WholeRawData.png'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.savefig(\"locust-sorting-python/WholeRawData.png\")\n",
    "\"locust-sorting-python/WholeRawData.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data renormalization\n",
    "\n",
    "We are going to use a median absolute deviation (MAD) based renormalization. The goal of the procedure is to scale the raw data such that the noise SD is approximately 1. Since it is not straightforward to obtain a noise SD on data where both signal (i.e., spikes) and noise are present, we use this robust type of statistic for the SD-And we normalize accordingly (we also subtract the median which is not exactly 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mad = list(map(swp.mad,data))\n",
    "data = list(map(lambda x: (x-np.median(x))/swp.mad(x), data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(tt,data[0],color=\"black\")\n",
    "plt.xlim([0,0.2])\n",
    "plt.ylim([-17,13])\n",
    "plt.axhline(y=1,color=\"red\")\n",
    "plt.axhline(y=-1,color=\"red\")\n",
    "plt.axhline(y=np.std(data[0]),color=\"blue\",linestyle=\"dashed\")\n",
    "plt.axhline(y=-np.std(data[0]),color=\"blue\",linestyle=\"dashed\")\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylim([-5,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the MAD does its job as a robust estimate of the noise standard deviation by looking at Q-Q plots of the whole traces normalized with the MAD and normalized with the \"classical\" SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Empirical quantiles')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "dataQ = map(lambda x:\n",
    "            mquantiles(x, prob=np.arange(0.01,0.99,0.001)),data)\n",
    "dataQsd = map(lambda x:\n",
    "              mquantiles(x/np.std(x), prob=np.arange(0.01,0.99,0.001)),\n",
    "              data)\n",
    "from scipy.stats import norm\n",
    "qq = norm.ppf(np.arange(0.01,0.99,0.001))\n",
    "plt.plot(np.linspace(-3,3,num=100),np.linspace(-3,3,num=100),\n",
    "         color='grey')\n",
    "colors = ['black', 'orange', 'blue', 'red']\n",
    "for i,y in enumerate(dataQ):\n",
    "    plt.plt.plot(qq,y,color=colors[i])\n",
    "\n",
    "for i,y in enumerate(dataQsd):\n",
    "    plt.plot(qq,y,color=colors[i],linestyle=\"dashed\")\n",
    "\n",
    "plt.xlabel('Normal quantiles')\n",
    "plt.ylabel('Empirical quantiles')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the behavior of the \"away from normal\" fraction is much more homogeneous for small, as well as for large in fact, quantile values with the MAD normalized traces than with the SD normalized ones. If we consider automatic rules like the three sigmas we are going to reject fewer events (i.e., get fewer putative spikes) with the SD based normalization than with the MAD based one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect peaks\n",
    "\n",
    "We are going to filter the data slightly using a \"box\" filter of length 3. That is, the data points of the original trace are going to be replaced by the average of themselves with their four nearest neighbors. We will then scale the filtered traces such that the MAD is one on each recording sites and keep only the parts of the signal which above 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import fftconvolve\n",
    "from numpy import apply_along_axis as apply \n",
    "data_filtered = apply(lambda x:\n",
    "                      fftconvolve(x,np.array([1,1,1,1,1])/5.,'same'),\n",
    "                      1,np.array(data))\n",
    "data_filtered = (data_filtered.transpose() / \\\n",
    "                 apply(swp.mad,1,data_filtered)).transpose()\n",
    "data_filtered[data_filtered < 4] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the difference between the raw trace and the filtered and rectified "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Time (s)')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(tt, data[0],color='black')\n",
    "plt.axhline(y=4,color=\"blue\",linestyle=\"dashed\")\n",
    "plt.plot(tt, data_filtered[0,],color='red')\n",
    "plt.xlim([0,0.2])\n",
    "plt.ylim([-5,10])\n",
    "plt.xlabel('Time (s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use function peak on the sum of the rows of our filtered and rectified version of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp0 = swp.peak(data_filtered.sum(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then check the detection quality with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "swp.plot_data_list_and_detection(data,tt,sp0)\n",
    "plt.xlim([0,0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the text, we want to \"emulate\" a long data set analysis where the model is estimated on the early part before doing template matching on what follows. We therefore get an \"early\" and a \"late\" part by splitting the data set in two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp0E = sp0[sp0 <= data_len/2.]\n",
    "sp0L = sp0[sp0 > data_len/2.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuts\n",
    "\n",
    "After detecting our spikes, we must make our cuts in order to create our events' sample. The obvious question we must first address is: How long should our cuts be? The pragmatic way to get an answer is:\n",
    "\n",
    "Make cuts much longer than what we think is necessary, like 50 sampling points on both sides of the detected event's time.\n",
    "Compute robust estimates of the \"central\" event (with the median) and of the dispersion of the sample around this central event (with the MAD).\n",
    "Plot the two together and check when does the MAD trace reach the background noise level (at 1 since we have normalized the data).\n",
    "Having the central event allows us to see if it outlasts significantly the region where the MAD is above the background noise level.\n",
    "Clearly cutting beyond the time at which the MAD hits back the noise level should not bring any useful information as far a classifying the spikes is concerned. So here we perform this task as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "evtsE = swp.mk_events(sp0E,np.array(data),49,50)\n",
    "evtsE_median=apply(np.median,0,evtsE)\n",
    "evtsE_mad=apply(swp.mad,0,evtsE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23436d491d0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(evtsE_median, color='red', lw=2)\n",
    "plt.axhline(y=0, color='black')\n",
    "for i in np.arange(0,400,100): \n",
    "    plt.axvline(x=i, color='black', lw=2)\n",
    "\n",
    "for i in np.arange(0,400,10): \n",
    "    plt.axvline(x=i, color='grey')\n",
    "\n",
    "plt.plot(evtsE_median, color='red', lw=2)\n",
    "plt.plot(evtsE_mad, color='blue', lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are satisfied with our spike detection, at least in a provisory way, and that we have decided on the length of our cuts, we proceed by making cuts around the detected events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "evtsE = swp.mk_events(sp0E,np.array(data),14,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "swp.plot_events(evtsE,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting an estimate of the noise statistical properties is an essential ingredient to build respectable goodness of fit tests. In our approach \"noise events\" are essentially anything that is not an \"event\" is the sense of the previous section. I wrote essentially and not exactly since there is a little twist here which is the minimal distance we are willing to accept between the reference time of a noise event and the reference time of the last preceding and of the first following \"event\". We could think that keeping a cut length on each side would be enough. That would indeed be the case if all events were starting from and returning to zero within a cut. But this is not the case with the cuts parameters we chose previously (that will become clear soon). You might wonder why we chose so short a cut length then. Simply to avoid having to deal with too many superposed events which are the really bothering events for anyone wanting to do proper sorting. To obtain our noise events we are going to use function mk_noise which takes the same arguments as function mk_events plus two numbers:\n",
    "\n",
    "safety_factor a number by which the cut length is multiplied and which sets the minimal distance between the reference times discussed in the previous paragraph.\n",
    "size the maximal number of noise events one wants to cut (the actual number obtained might be smaller depending on the data length, the cut length, the safety factor and the number of events).\n",
    "We cut noise events with a rather large safety factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "noiseE = swp.mk_noise(sp0E,np.array(data),14,30,safety_factor=2.5,size=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our spike sorting has two main stages, the first one consist in estimating a model and the second one consists in using this model to classify the data. Our model is going to be built out of reasonably \"clean\" events. Here by clean we mean events which are not due to a nearly simultaneous firing of two or more neurons; and simultaneity is defined on the time scale of one of our cuts. When the model will be subsequently used to classify data, events are going to decomposed into their (putative) constituent when they are not \"clean\", that is, superposition are going to be looked and accounted for.\n",
    "\n",
    "In order to eliminate the most obvious superpositions we are going to use a rather brute force approach, looking at the sides of the central peak of our median event and checking if individual events are not too large there, that is do not exhibit extra peaks. We first define a function doing this job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_evts_fct(samp, thr=3):\n",
    "    samp_med = apply(np.median,0,samp)\n",
    "    samp_mad = apply(swp.mad,0,samp)\n",
    "    above = samp_med > 0\n",
    "    samp_r = samp.copy()\n",
    "    for i in range(samp.shape[0]): samp_r[i,above] = 0\n",
    "    samp_med[above] = 0\n",
    "    res = apply(lambda x:\n",
    "                np.all(abs((x-samp_med)/samp_mad) < thr),\n",
    "                1,samp_r)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply our new function to our sample using a threshold of 8 (set by trial and error)\n",
    "Out of len(goodEvts) 898 events we get sum(goodEvts) 843 \"good\" ones. As usual, the first 200 good ones can be visualized with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodEvts = good_evts_fct(evtsE,8)\n",
    "plt.figure()\n",
    "swp.plot_events(evtsE[goodEvts,:][:200,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"bad\" guys can be visualized with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "swp.plot_events(evtsE[~goodEvts,:],\n",
    "                show_median=False,\n",
    "                show_mad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension reduction \n",
    "\n",
    "Our events are living right now in an 180 dimensional space (our cuts are 45 sampling points long and we are working with 4 recording sites simultaneously). It turns out that it hard for most humans to perceive structures in such spaces. It also hard, not to say impossible with a realistic sample size, to estimate probability densities (which is what model based clustering algorithms are actually doing) in such spaces, unless one is ready to make strong assumptions about these densities. It is therefore usually a good practice to try to reduce the dimension of the sample space used to represent the data. We are going to that with principal component analysis (PCA), using it on our \"good\" events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import svd\n",
    "varcovmat = np.cov(evtsE[goodEvts,:].T)\n",
    "u, s, v = svd(varcovmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this \"back to the roots\" approach, u should be an orthonormal matrix whose column are made of the principal components (and v should be the transpose of u since our matrix varcovmat is symmetric and real by construction). s is a vector containing the amount of sample variance explained by each principal component.\n",
    "PCA is a rather abstract procedure to most of its users, at least when they start using it. But one way to grasp what it does is to plot the mean event plus or minus, say five times, each principal components like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "evt_idx = range(180)\n",
    "evtsE_good_mean = np.mean(evtsE[goodEvts,:],0)\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.plot(evt_idx,evtsE_good_mean, 'black',evt_idx,\n",
    "             evtsE_good_mean + 5 * u[:,i],\n",
    "             'red',evt_idx,evtsE_good_mean - 5 * u[:,i], 'blue')\n",
    "    plt.title('PC' + str(i) + ': ' + str(round(s[i]/sum(s)*100)) +'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first 3 PCs correspond to pure amplitude variations. An event with a large projection (score) on the first PC is smaller than the average event on recording sites 1, 2 and 3, but not on 4. An event with a large projection on PC 1 is larger than average on site 1, smaller than average on site 2 and 3 and identical to the average on site 4. An event with a large projection on PC 2 is larger than the average on site 4 only. PC 3 is the first principal component corresponding to a change in shape as opposed to amplitude. A large projection on PC 3 means that the event as a shallower first valley and a deeper second valley than the average event on all recording sites.\n",
    "We now look at the next 4 principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(4,8):\n",
    "    plt.subplot(2,2,i-3)\n",
    "    plt.plot(evt_idx,evtsE_good_mean, 'black',\n",
    "             evt_idx,evtsE_good_mean + 5 * u[:,i], 'red',\n",
    "             evt_idx,evtsE_good_mean - 5 * u[:,i], 'blue')\n",
    "    plt.title('PC' + str(i) + ': ' + str(round(s[i]/sum(s)*100)) +'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An event with a large projection on PC 4 tends to be \"slower\" than the average event. An event with a large projection on PC 5 exhibits a slower kinetics of its second valley than the average event. PC 4 and 5 correspond to effects shared among recording sites. PC 6 correspond also to a \"change of shape\" effect on all sites except the first. Events with a large projection on PC 7 rise slightly faster and decay slightly slower than the average event on all recording site. Notice also that PC 7 has a \"noisier\" aspect than the other suggesting that we are reaching the limit of the \"events extra variability\" compared to the variability present in the background noise.\n",
    "\n",
    "This guess can be confirmed by comparing the variance of the \"good\" events sample with the one of the noise sample to which the variance contributed by the first K PCs is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, -577.5515048194728),\n",
       " (1, -277.46515432919693),\n",
       " (2, -187.56341162342244),\n",
       " (3, -128.03907765900965),\n",
       " (4, -91.31866909961741),\n",
       " (5, -58.83988760231364),\n",
       " (6, -36.36306744692422),\n",
       " (7, -21.543722414005288),\n",
       " (8, -8.264495177520416),\n",
       " (9, 0.2848892942456587),\n",
       " (10, 6.906733550093577),\n",
       " (11, 13.341548838374706),\n",
       " (12, 19.472089099227333),\n",
       " (13, 25.255335647533684),\n",
       " (14, 29.102104713041854)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noiseVar = sum(np.diag(np.cov(noiseE.T)))\n",
    "evtsVar = sum(s)\n",
    "[(i,sum(s[:i])+noiseVar-evtsVar) for i in range(15)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that keeping the first 10 PCs should be more than enough\n",
    "We can build a scatter plot matrix showing the projections of our \"good\" events sample onto the plane defined by pairs of the few first PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "evtsE_good_P0_to_P3 = np.dot(evtsE[goodEvts,:],u[:,0:4])\n",
    "from pandas.plotting import scatter_matrix\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(evtsE_good_P0_to_P3)\n",
    "scatter_matrix(df,alpha=0.2,s=4,c='k',figsize=(6,6),\n",
    "               diagonal='kde',marker=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to discern structures in \"high dimensional\" data is to dynamically visualize them. To this end, the tool of choice is GGobi, an open source software available on Linux, Windows and MacOS. We start by exporting our data in csv format to our disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "f = open('evtsE.csv','w')\n",
    "w = csv.writer(f)\n",
    "w.writerows(np.dot(evtsE[goodEvts,:],u[:,:8]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following terse procedure should allow the reader to get going with GGobi:\n",
    "\n",
    "Launch GGobi\n",
    "In menu: File -> Open, select evtsE.csv.\n",
    "Since the glyphs are rather large, start by changing them for smaller ones:\n",
    "Go to menu: Interaction -> Brush.\n",
    "On the Brush panel which appeared check the Persistent box.\n",
    "Click on Choose color & glyph....\n",
    "On the chooser which pops out, click on the small dot on the upper left of the left panel.\n",
    "Go back to the window with the data points.\n",
    "Right click on the lower right corner of the rectangle which appeared on the figure after you selected Brush.\n",
    "Dragg the rectangle corner in order to cover the whole set of points.\n",
    "Go back to the Interaction menu and select the first row to go back where you were at the start.\n",
    "Select menu: View -> Rotation.\n",
    "Adjust the speed of the rotation in order to see things properly.\n",
    "We easily discern 10 rather well separated clusters. Meaning that an automatic clustering with 10 clusters on the first 3 principal components should do the job.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with K-Means\n",
    "\n",
    "Since our dynamic visualization shows 10 well separated clusters in 3 dimension, a simple k-means should do the job. We are using here the KMeans class of scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km10 = KMeans(n_clusters=10, init='k-means++', n_init=100, max_iter=100)\n",
    "km10.fit(np.dot(evtsE[goodEvts,:],u[:,0:3]))\n",
    "c10 = km10.fit_predict(np.dot(evtsE[goodEvts,:],u[:,0:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to facilitate comparison when models with different numbers of clusters or when different models are used, clusters are sorted by \"size\". The size is defined here as the sum of the absolute value of the median of the cluster (an L1 norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_median = list([(i,\n",
    "                        np.apply_along_axis(np.median,0,\n",
    "                                            evtsE[goodEvts,:][c10 == i,:]))\n",
    "                                            for i in range(10)\n",
    "                                            if sum(c10 == i) > 0])\n",
    "cluster_size = list([np.sum(np.abs(x[1])) for x in cluster_median])\n",
    "new_order = list(reversed(np.argsort(cluster_size)))\n",
    "new_order_reverse = sorted(range(len(new_order)), key=new_order.__getitem__)\n",
    "c10b = [new_order_reverse[i] for i in c10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Looking at the different clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-15, 20)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.subplot(511)\n",
    "swp.plot_events(evtsE[goodEvts,:][np.array(c10b) == 0,:])\n",
    "plt.ylim([-15,20])\n",
    "plt.subplot(512)\n",
    "swp.plot_events(evtsE[goodEvts,:][np.array(c10b) == 1,:])\n",
    "plt.ylim([-15,20])\n",
    "plt.subplot(513)\n",
    "swp.plot_events(evtsE[goodEvts,:][np.array(c10b) == 2,:])\n",
    "plt.ylim([-15,20])\n",
    "plt.subplot(514)\n",
    "swp.plot_events(evtsE[goodEvts,:][np.array(c10b) == 3,:])\n",
    "plt.ylim([-15,20])\n",
    "plt.subplot(515)\n",
    "swp.plot_events(evtsE[goodEvts,:][np.array(c10b) == 4,:])\n",
    "plt.ylim([-15,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-10, 10)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.subplot(511)\n",
    "swp.plot_events(evtsE[goodEvts,:][np.array(c10b) == 5,:])\n",
    "plt.ylim([-10,10])\n",
    "plt.subplot(512)\n",
    "swp.plot_events(evtsE[goodEvts,:][np.array(c10b) == 6,:])\n",
    "plt.ylim([-10,10])\n",
    "plt.subplot(513)\n",
    "swp.plot_events(evtsE[goodEvts,:][np.array(c10b) == 7,:])\n",
    "plt.ylim([-10,10])\n",
    "plt.subplot(514)\n",
    "swp.plot_events(evtsE[goodEvts,:][np.array(c10b) == 8,:])\n",
    "plt.ylim([-10,10])\n",
    "plt.subplot(515)\n",
    "swp.plot_events(evtsE[goodEvts,:][np.array(c10b) == 9,:])\n",
    "plt.ylim([-10,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by checking our clustering quality with GGobi. To this end we export the data and the labels of each event:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('evtsEsorted.csv','w')\n",
    "w = csv.writer(f)\n",
    "w.writerows(np.concatenate((np.dot(evtsE[goodEvts,:],u[:,:8]),\n",
    "                            np.array([c10b]).T),\n",
    "                            axis=1))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An again succinct description of how to do the dynamical visual check is:\n",
    "\n",
    "Load the new data into GGobi like before.\n",
    "In menu: Display -> New Scatterplot Display, select evtsEsorted.csv.\n",
    "Change the glyphs like before.\n",
    "In menu: Tools -> Color Schemes, select a scheme with 10 colors, like Spectral, Spectral 10.\n",
    "In menu: Tools -> Automatic Brushing, select evtsEsorted.csv tab and, within this tab, select variable c10b. Then click on Apply.\n",
    "Select View -> Rotation like before and see your result."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 10 Spike \"peeling\": a \"Brute force\" superposition resolution\n",
    "\n",
    "We are going to resolve (the most \"obvious\") superpositions by a \"recursive peeling method\":\n",
    "\n",
    "Events are detected and cut from the raw data or from an already peeled version of the data.\n",
    "The closest center (in term of Euclidean distance) to the event is found.\n",
    "If the residual sum of squares (RSS), that is: (actual data - best center)2, is smaller than the squared norm of a cut, the best center is subtracted from the data on which detection was performed—jitter is again compensated for at this stage.\n",
    "Go back to step 1 or stop.\n",
    "To apply this procedure, we need, for each cluster, estimates of its center and of its first two derivatives. Function mk_center_dictionary does the job for us. We must moreover build our clusters' centers such that they can be used for subtraction, this implies that we should make them long enough, on both side of the peak, to see them go back to baseline. Formal parameters before and after bellow should therefore be set to larger values than the ones used for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = { \"Cluster \" + str(i) :\n",
    "            swp.mk_center_dictionary(sp0E[goodEvts][np.array(c10b)==i],\n",
    "                                     np.array(data))\n",
    "            for i in range(10)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function classify_and_align_evt is used next. For each detected event, it matches the closest template, correcting for the jitter, if the closest template is close enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cluster 7', 281, -0.14107833394834735]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swp.classify_and_align_evt(sp0[0],np.array(data),centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function on every detected event. A trick here is to store the matrix version of the data in order to avoid the conversion of the list of vectors (making the data of the different channels) into a matrix for each detected event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = np.array(data) \n",
    "round0 = [swp.classify_and_align_evt(sp0[i],data0,centers)\n",
    "          for i in range(len(sp0))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how many events got unclassified "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x[1] for x in round0 if x[0] == '?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using function predict_data, we create an ideal data trace given events' positions, events' origins and a clusters' catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred0 = swp.predict_data(round0,centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then subtract the prediction (pred0) from the data (data0) to get the \"peeled\" data (data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data0 - pred0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the original data with the result of the \"first peeling\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(tt, data0[0,], color='black')\n",
    "plt.plot(tt, data1[0,], color='red',lw=0.3)\n",
    "plt.plot(tt, data0[1,]-15, color='black')\n",
    "plt.plot(tt, data1[1,]-15, color='red',lw=0.3)\n",
    "plt.plot(tt, data0[2,]-25, color='black')\n",
    "plt.plot(tt, data1[2,]-25, color='red',lw=0.3)\n",
    "plt.plot(tt, data0[3,]-40, color='black')\n",
    "plt.plot(tt, data1[3,]-40, color='red',lw=0.3)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.xlim([0.9,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second peeling\n",
    "\n",
    "We then take data1 as our former data0 and we repeat the procedure. We do it with slight modifications: detection is done on a single recording site and a shorter filter length is used before detecting the events. Doing detection on a single site (here site 0) allows us to correct some drawbacks of our crude spike detection method. When we used it the first time we summed the filtered and rectified versions of the data before looking at peaks. This summation can lead to badly defined spike times when two neurons that are large on different recording sites, say site 0 and site 1 fire at nearly the same time. The summed event can then have a peak in between the two true peaks and our jitter correction cannot resolve that. We are therefore going to perform detection on the different sites. The jitter estimation and the subtraction are always going to be done on the 4 recording sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered = np.apply_along_axis(lambda x:\n",
    "                                    fftconvolve(x,np.array([1,1,1])/3.,\n",
    "                                                'same'),\n",
    "                                    1,dadta1)\n",
    "data_filtered = (data_filtered.transpose() /\n",
    "                 np.apply_along_axis(swp.mad,1,\n",
    "                                     data_filtered)).transpose()\n",
    "data_filtered[data_filtered < 4] = 0\n",
    "sp1 = swp.peak(data_filtered[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We classify the events and obtain the new prediction and the new \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "round1 = [swp.classify_and_align_evt(sp1[i],data1,centers)\n",
    "          for i in range(len(sp1))]\n",
    "pred1 = swp.predict_data(round1,centers)\n",
    "data2 = data1 - pred1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how many events got unclassified "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x[1] for x in round1 if x[0] == '?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the first peeling with the second one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(tt, data1[0,], color='black')\n",
    "plt.plot(tt, data2[0,], color='red',lw=0.3)\n",
    "plt.plot(tt, data1[1,]-15, color='black')\n",
    "plt.plot(tt, data2[1,]-15, color='red',lw=0.3)\n",
    "plt.plot(tt, data1[2,]-25, color='black')\n",
    "plt.plot(tt, data2[2,]-25, color='red',lw=0.3)\n",
    "plt.plot(tt, data1[3,]-40, color='black')\n",
    "plt.plot(tt, data2[3,]-40, color='red',lw=0.3)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.xlim([0.9,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third peeling\n",
    "We take data2 as our former data1 and we repeat the procedure detecting on channel 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered = apply(lambda x:\n",
    "                      fftconvolve(x,np.array([1,1,1])/3.,'same'),\n",
    "                      1,data2)\n",
    "data_filtered = (data_filtered.transpose() / \\\n",
    "                 apply(swp.mad,1,data_filtered)).transpose()\n",
    "data_filtered[data_filtered < 4] = 0\n",
    "sp2 = swp.peak(data_filtered[1,:])\n",
    "len(sp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification follows with the prediction and the number of unclassified events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round2 = [swp.classify_and_align_evt(sp2[i],data2,centers) for i in range(len(sp2))]\n",
    "pred2 = swp.predict_data(round2,centers)\n",
    "data3 = data2 - pred2\n",
    "len([x[1] for x in round2 if x[0] == '?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the second peeling with the third one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(tt, data2[0,], color='black')\n",
    "plt.plot(tt, data3[0,], color='red',lw=0.3)\n",
    "plt.plot(tt, data2[1,]-15, color='black')\n",
    "plt.plot(tt, data3[1,]-15, color='red',lw=0.3)\n",
    "plt.plot(tt, data2[2,]-25, color='black')\n",
    "plt.plot(tt, data3[2,]-25, color='red',lw=0.3)\n",
    "plt.plot(tt, data2[3,]-40, color='black')\n",
    "plt.plot(tt, data3[3,]-40, color='red',lw=0.3)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.xlim([0.9,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered = apply(lambda x:\n",
    "                      fftconvolve(x,np.array([1,1,1])/3.,'same'),\n",
    "                      1,data3)\n",
    "data_filtered = (data_filtered.transpose() / \\\n",
    "                 apply(swp.mad,1,data_filtered)).transpose()\n",
    "data_filtered[data_filtered < 4] = 0\n",
    "sp3 = swp.peak(data_filtered[2,:])\n",
    "len(sp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round3 = [swp.classify_and_align_evt(sp3[i],data3,centers) for i in range(len(sp3))]\n",
    "pred3 = swp.predict_data(round3,centers)\n",
    "data4 = data3 - pred3\n",
    "len([x[1] for x in round3 if x[0] == '?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.9, 4)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(tt, data3[0,], color='black')\n",
    "plt.plot(tt, data4[0,], color='red',lw=0.3)\n",
    "plt.plot(tt, data3[1,]-15, color='black')\n",
    "plt.plot(tt, data4[1,]-15, color='red',lw=0.3)\n",
    "plt.plot(tt, data3[2,]-25, color='black')\n",
    "plt.plot(tt, data4[2,]-25, color='red',lw=0.3)\n",
    "plt.plot(tt, data3[3,]-40, color='black')\n",
    "plt.plot(tt, data4[3,]-40, color='red',lw=0.3)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.xlim([3.9,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered = apply(lambda x:\n",
    "                      fftconvolve(x,np.array([1,1,1])/3.,'same'),\n",
    "                      1,data4)\n",
    "data_filtered = (data_filtered.transpose() / \\\n",
    "                 apply(swp.mad,1,data_filtered)).transpose()\n",
    "data_filtered[data_filtered < 4] = 0\n",
    "sp4 = swp.peak(data_filtered[3,:])\n",
    "len(sp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round4 = [swp.classify_and_align_evt(sp4[i],data4,centers) for i in range(len(sp4))]\n",
    "pred4 = swp.predict_data(round4,centers)\n",
    "data5 = data4 - pred4\n",
    "len([x[1] for x in round4 if x[0] == '?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.9, 4)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(tt, data4[0,], color='black')\n",
    "plt.plot(tt, data5[0,], color='red',lw=0.3)\n",
    "plt.plot(tt, data4[1,]-15, color='black')\n",
    "plt.plot(tt, data5[1,]-15, color='red',lw=0.3)\n",
    "plt.plot(tt, data4[2,]-25, color='black')\n",
    "plt.plot(tt, data5[2,]-25, color='red',lw=0.3)\n",
    "plt.plot(tt, data4[3,]-40, color='black')\n",
    "plt.plot(tt, data5[3,]-40, color='red',lw=0.3)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.xlim([3.9,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General comparison\n",
    "We can compare the raw data with the fifth peeling on the first second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(tt, data0[0,], color='black')\n",
    "plt.plot(tt, data5[0,], color='red',lw=0.3)\n",
    "plt.plot(tt, data0[1,]-15, color='black')\n",
    "plt.plot(tt, data5[1,]-15, color='red',lw=0.3)\n",
    "plt.plot(tt, data0[2,]-25, color='black')\n",
    "plt.plot(tt, data5[2,]-25, color='red',lw=0.3)\n",
    "plt.plot(tt, data0[3,]-40, color='black')\n",
    "plt.plot(tt, data5[3,]-40, color='red',lw=0.3)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.xlim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the remaining unclassified events; they don't look like any of our templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "bad_ones = [x[1] for x in round4 if x[0] == '?']\n",
    "r4BE = swp.mk_events(bad_ones, data4)\n",
    "swp.plot_events(r4BE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the spike trains\n",
    "\n",
    "Once we have decided to stop the peeling iterations we can extract our spike trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_all = round0.copy() \n",
    "round_all.extend(round1)\n",
    "round_all.extend(round2)\n",
    "round_all.extend(round3)\n",
    "round_all.extend(round4)\n",
    "spike_trains = { n : np.sort([x[1] + x[2] for x in round_all\n",
    "                              if x[0] == n]) for n in list(centers)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of spikes attributed to each neuron is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Cluster 0', 92),\n",
       " ('Cluster 1', 173),\n",
       " ('Cluster 2', 101),\n",
       " ('Cluster 3', 173),\n",
       " ('Cluster 4', 63),\n",
       " ('Cluster 5', 149),\n",
       " ('Cluster 6', 238),\n",
       " ('Cluster 7', 233),\n",
       " ('Cluster 8', 456),\n",
       " ('Cluster 9', 588)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(n,len(spike_trains[n])) for n in list(centers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual function definitions\n",
    "\n",
    "Short function are presented in 'one piece'. The longer ones are presented with their docstring first followed by the body of the function. To get the actual function you should replace the <<docstring>> appearing in the function definition by the actual doctring. This is just a direct application of the literate programming paradigm. More complicated functions are split into more parts with their own descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
